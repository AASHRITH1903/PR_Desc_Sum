{
  "elastic/elasticsearch_37945": {
    "id": "elastic/elasticsearch_37945",
    "body": "'Adding release highlights for ES . '",
    "cms": [
      "'add es release highlights'",
      "'Update docs/reference/release-notes/highlights-6.6.0.asciidoc\\n\\nCo-Authored-By: pcsanwald <paul.sanwald@elastic.co>'",
      "'Update docs/reference/release-notes/highlights-6.6.0.asciidoc\\n\\nCo-Authored-By: pcsanwald <paul.sanwald@elastic.co>'",
      "'Update docs/reference/release-notes/highlights-6.6.0.asciidoc\\n\\nCo-Authored-By: pcsanwald <paul.sanwald@elastic.co>'",
      "'Update docs/reference/release-notes/highlights-6.6.0.asciidoc\\n\\nCo-Authored-By: pcsanwald <paul.sanwald@elastic.co>'",
      "'Update docs/reference/release-notes/highlights-6.6.0.asciidoc\\n\\nCo-Authored-By: pcsanwald <paul.sanwald@elastic.co>'",
      "'Update docs/reference/release-notes/highlights-6.6.0.asciidoc\\n\\nCo-Authored-By: pcsanwald <paul.sanwald@elastic.co>'"
    ],
    "commits": {
      "'6d643fd6cf38dbbf7a0a201c87901a299b5f20e9'": {
        "cm": "'add es release highlights '",
        "comments": ""
      },
      "'0f82c284d002eb8efe8dab60f37891266850ef6d'": {
        "cm": "",
        "comments": ""
      },
      "'aa36ad0f8f1ec8b2f34a1eb9dc5fcf4aa9c7c50d'": {
        "cm": "",
        "comments": ""
      },
      "'a728268833bb6a0ba637ccf73d63726c5329ede0'": {
        "cm": "",
        "comments": ""
      },
      "'7ba43435eb7fab49970f3691d01513d31e44db07'": {
        "cm": "",
        "comments": ""
      },
      "'c69b4a799674c091d8cb5acf460a6427ccf158b1'": {
        "cm": "",
        "comments": ""
      },
      "'589e66d4d1cbc900fe3eab61d0b93cc8e2f61a8d'": { "cm": "", "comments": "" }
    }
  },
  "elastic/elasticsearch_37872": {
    "id": "elastic/elasticsearch_37872",
    "body": "'The update request has a lesser known support for a one off update of a known document version .",
    "cms": [
      "'wip'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'",
      "'add rest test'",
      "'lint'",
      "'fix write issue'",
      "'sigh'",
      "'wires'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'",
      "'sigh'",
      "'numbers are hard'",
      "'rest fix'",
      "'some assertions'",
      "'wrong field'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'",
      "'feedback'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'",
      "'conditional'",
      "'Merge remote-tracking branch 'upstream/master' into cas_update_req'"
    ],
    "commits": {
      "'d297d875fff4bf1350bf75b13651fcc681a49ead'": {
        "cm": "'wip '",
        "comments": "only perform this update request if the document was last modification was assigned the given sequence number . only performs this update request if the document was last modification was assigned the given primary term . If set , only perform this update request if the document was last modification was assigned this sequence number . If set , only perform this update request if the document was last modification was assigned this primary term . only perform this update request if the document was last modification was assigned the given sequence number . only perform this update request if the document was last modification was assigned the given primary term ."
      },
      "'41632be268189769ddf0ffeefed37a304b701bfa'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "some test projects do n't have a main source set conventions are not honored when the tasks are disabled we need to specify the exact version of build-tools because gradle automatically adds its plugin portal which appears to mirror jcenter , opening us up to pulling a \\'later\\ ' version of build-tools Creates an index using the Create Index API . Asynchronously creates an index using the Create Index API . Retrieves the field mappings on an index or indices using the Get Field Mapping API . Constructs a new request to create an index with the specified name . The name of the index to create . The settings to create the index with . The settings to create the index with . The settings to create the index with . The settings to create the index with ( either json or yaml format ) Allows to set the settings using a json builder . The settings to create the index with ( either json/yaml/properties format ) Adds mapping that will be added when the index gets created . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the number of shard copies that should be active for index creation to return . Index creation will only wait up until the timeout value for the number of shard copies to be active before returning . Indicate whether the receiving node should operate based on local index information or forward requests , where needed , to other nodes . If running locally , request will not raise errors if running locally & amp ; missing indices . Returns the fields mapping . The return map keys are indexes and fields ( as specified in the request ) . Returns the mappings of a specific index and field . Returns the mappings as a map . pkg-private for testing Create index Create index with mappings , aliases and settings This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation Create leader index : Follow index , so that we can query for follow stats : 0 system roles plus the three we created As Alias # equals only looks at name , we check the equality of the other Alias parameters here . Randomizes the index name , the aliases , mappings and settings associated with the index . When present , the mappings make no mention of types . Create a random server request , and copy its contents into the HLRC request . Because client requests only accept typeless mappings , we must swap out the mapping definition for one that does not contain types . Creates a random mapping , with no mention of types . metadata is a series of kv pairs , so we dont want to add random fields here for test equality"
      },
      "'cb585db70e3bc7ac1a80cd85c33f6050dd9470da'": {
        "cm": "'add rest test '",
        "comments": ""
      },
      "'e7eb0a73e8e4e6c6d23fe42fa6fd412aed99865e'": {
        "cm": "'lint '",
        "comments": ""
      },
      "'11d3986c86f4c5523135c164048874c309da68f9'": {
        "cm": "'fix write issue '",
        "comments": ""
      },
      "'1b4fbf1fc6e87c9a836210bc3ceeb1fc02c1e510'": {
        "cm": "'sigh '",
        "comments": ""
      },
      "'8d2a1a553e2141856a44c129946a5300cf5cee9a'": {
        "cm": "'wires '",
        "comments": ""
      },
      "'2bb0b0e04e6d398e8c55467e9037b7b9bc7c355a'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "package private for tests validate the join on the joining node , will throw a failure if it fails the validation"
      },
      "'4cb6b60ffb872249cf35d5d24317542cfa13c78a'": {
        "cm": "'sigh '",
        "comments": ""
      },
      "'dc89962aa2e73484783db5488a3f2fb44d321f77'": {
        "cm": "'numbers are hard '",
        "comments": ""
      },
      "'96dd81ee66b7fe98473a50ef0a731d5e80846e99'": {
        "cm": "'rest fix '",
        "comments": ""
      },
      "'ff6c34e90c808eed01223bcb86f6a186bab44f12'": {
        "cm": "'some assertions '",
        "comments": ""
      },
      "'2b8f9f84578514ca5271fbf597ad528ee7202e38'": {
        "cm": "'wrong field '",
        "comments": ""
      },
      "'46fd7da8ded96f015bb8d1a909bb3d2eda84c4b7'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "Returns null if the provided factory and his parents are compatible with this aggregator or the instance of the parent 's factory that is incompatible with the composite aggregation . Returns the total hit count that should be tracked or null if the value is unset . Defaults to null . Returns true if this collector has early terminated . client ( ) also starts the node build settings using same path.data as original but with node.data=false and node.master=false test that we can create data=false and master=false with no meta information test that we can create data=false env with only meta information . Also create shard data for following asserts assert that we get the stricter message on meta-data when both conditions fail build settings using same path.data as original but with node.master=false test that we can create master=false env regardless of data . test that we can create data=true , master=true env . Also remove state dir to leave only shard data for following asserts assert that we fail on shard data even without the metadata dir ."
      },
      "'591704620e89559533a29ff99126b782677a074d'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "Retrieves the mappings on an index or indices using the Get Mapping API . Asynchronously retrieves the mappings on an index on indices using the Get Mapping API . Indicates whether the receiving node should operate based on local index information or forward requests , where needed , to other nodes . If running locally , request will not raise errors if local index information is missing . 0 system roles plus the three we created Because the client-side class does not have a toXContent method , we test xContent serialization by creating a random client object , converting it to a server object then serializing it to xContent , and finally parsing it back as a client object . We check equality between the original client object , and the parsed one . Flatten multipoints A callback when a new retention lease is created or an existing retention lease expires . In practice , this callback invokes the retention lease sync action , to sync retention leases to replicas . the primary calculates the non-expired retention leases and syncs them to replicas early out as no retention leases have expired clean up the expired retention leases At this point , we were either in primary mode and have updated the non-expired retention leases into the tracking map , or we were in replica mode and merely need to copy the existing retention leases since a replica does not calculate the non-expired retention leases , instead receiving them on syncs from the primary . Renews an existing retention lease . This action is deliberately a write action so that if a replica misses a retention lease sync then that shard will be marked as stale . Sync the specified retention leases for the specified shard . The callback is invoked when the sync succeeds or fails . Represents a method that when invoked syncs retention leases to replica shards after a new retention lease is added on the primary . The specified listener is invoked when the syncing completes with success or failure . An unmodifiable copy of the retention leases is returned . The new typeless parsing is implemented in the client side GetMappingsResponse . Existent shard id but different allocation id Non existent shard id we do not want to hold a lock on the replication tracker in the callback ! assert that the new retention lease callback was invoked reset the invocation marker so that we can assert the callback was not invoked when renewing the lease leases do not expire on replicas until synced from the primary we do not want to hold a lock on the replication tracker in the callback ! assert that the new retention lease callback was invoked reset the invocation marker so that we can assert the callback was not invoked when renewing the lease reset the invocation marker so that we can assert the callback was invoked if any leases are expired randomly expire some leases calculate the expired leases and update our tracking map getting the leases has the side effect of calculating which leases are expired and invoking the sync callback the current leases should equal our tracking map the callback should only be invoked if there were expired leases retention leases can be expired on replicas , so we can only assert on primaries here the retention leases on the shard should be flushed we should forward the request containing the current retention leases to the replica we should start with an empty replication response the retention leases on the shard should be updated the retention leases on the shard should be flushed the result should indicate success execution happens on the test thread , so no need to register an actual listener to callback we will add multiple retention leases and expect to see them synced to all replicas check retention leases have been committed on the primary check current retention leases have been synced to all replicas check retention leases have been committed on the replica we will add multiple retention leases , wait for some to expire , and assert a consistent view between the primary and the replicas check current retention leases have been synced to all replicas sleep long enough that * possibly * the current retention lease has expired , and certainly that any previous have Check that expiration of retention leases has been synced to all replicas . We have to assert busy since syncing happens in the background . retention leases can be expired on replicas , so we can only assert on primaries here Always true because we include 'other ' in the agg Generate a random precision according to the rules of the given aggregation . Encode longitude and latitude with a given precision as a long hash . Generate a random precision according to the rules of the given aggregation . Make it easy to detect this error in ShardFollowNodeTask : ( adding a metadata header instead of introducing a new exception that extends ElasticsearchException ) Do restore from repository here and after that start ( ) should be invoked and stats should be reset For now handle like any other failure : need a more robust approach to avoid the scenario where an outstanding request can trigger another restore while the shard was restored already . Make sure at least one read-request which requires mapping sync is completed . Make the job 's results span an extra two indices , i.e . three in total . To do this the job 's results alias needs to encompass all three indices . Assert appropriate task state and assignment numbers Set the upgrade mode setting Assert state for tasks still exists and that the upgrade setting is set Disable the setting Step 0 . If we did not drop the indices and after DBQ state done , we delete the aliases Step 0 . If we did not delete the indices , we run a delete by query Step 0 . If we have any hits , that means we are NOT the only job on these indices , and should not delete the indices . If we do not have any hits , we can drop the indices and then skip the DBQ and alias deletion . Step 0 . Determine if we are on shared indices by looking at whether the initial index was \\'.ml-anomalies-shared\\ ' or whether the indices that the job 's results alias points to contain any documents from other jobs . TODO : this check is currently assuming that a job 's results indices are either ALL shared or ALL dedicated to the job . We have considered functionality like rolling jobs that generate large volumes of results from shared to dedicated indices . On deletion such a job would have a mix of shared indices requiring DBQ and dedicated indices that could be simply dropped . The current functionality would apply DBQ to all these indices , which is safe but suboptimal . So this functionality should be revisited when we add rolling results index functionality , especially if we add the ability to switch a job over to a dedicated index for future results . The job may no longer be using the initial shared index , but if it started off on a shared index then it will still be on a shared index even if it 's been reindexed do n't bother searching the index any further , we are on the default shared do n't bother searching the index any further - it 's already been closed or deleted Step 0 . Get the job as the initial result index name is required If we are waiting for an upgrade to complete , we should not assign to a node Do n't want folks spamming this endpoint while it is in progress , only allow one request to be handled at a time Noop , nothing for us to do , simply return fast to the caller < 0 > We have unassigned the tasks , respond to the listener . Wait for our tasks to all stop < 0 > After isolating the datafeeds , unassign the tasks Before ` upgrade_mode : true ` , there were unassigned tasks because node task assignment was maxed out ( tasks A , B ) There were assigned tasks executing fine ( tasks C , D ) While ` upgrade_mode : true ` all are attempting to be re-assigned , but can not and end up with the AWAITING_UPGRADE reason ` upgrade_mode : false ` opens the flood gates , all tasks are still attempting to re-assign A or B could be re-assigned before either C or D. Thus , previously erred tasks are now executing fine , and previously State change was not acknowledged , we either timed out or ran into some exception We should not continue and alert failure to the end user Did we change from disabled - > enabled ? Wait for jobs to not be \\'Awaiting upgrade\\ ' Datafeeds to wait for a non-\\'Awaiting upgrade\\ ' assignment and for the job task allocations to converge If we do not wait , deleting datafeeds , or attempting to unallocate them again causes issues as the job 's task allocationId could have changed during either process . < 0 > Change MlMetadata to indicate that upgrade_mode is now enabled Unassigns all Job and Datafeed tasks . < p > The reason for unassigning both types is that we want the Datafeed to attempt re-assignment once ` upgrade_mode ` is disabled . < p > If we do not force an allocation change for the Datafeed tasks , they will never start again , since they were isolated . Eventually we 'll probably just remove the checks that the Joda formats are valid , and at that point this method can be removed too . This method is using the Joda BWC layer . When that 's removed , this method can be deleted - we 'll just validate the Java time formats after that . Also remove enableWarningsCheck ( ) above if this method is removed . user_d can view repos and create and view snapshots on existings repos , everything else is DENIED user_d can create snapshots , but not concurrently view repositories view all indices , including restricted ones create snapshot that includes restricted indices view snapshots for repo try search all try create index try create another repo try delete repo try fumble with snapshots try destructive/revealing actions on all indices Use a custom index because other rolling upgrade tests meddle with the shared index The name of the concrete index underlying the results index alias may or may not have been changed by the upgrade process ( depending on what other tests are being run and the order they 're run in ) , so navigating to the next level of the tree must account for both cases"
      },
      "'a7c75e5853f0047452d48cdeda7b90ec3c261497'": {
        "cm": "'feedback '",
        "comments": "only perform this request if the document was last modification was assigned the given sequence number . only performs this request if the document was last modification was assigned the given primary term . If set , only perform this request if the document was last modification was assigned this sequence number . If set , only perform this request if the document was last modification was assigned this primary term ."
      },
      "'536efacffe8f402b2555be03ee7f90b727a67d35'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "The injected Unfollow step will run pretty rapidly here , so we need to wait for it to settle into the \\'stable\\ ' step of waiting to be ready to roll over tag : :ilm-explain-lifecycle-execute end : :ilm-explain-lifecycle-execute tag : :ilm-explain-lifecycle-response end : :ilm-explain-lifecycle-response Create a policy with just a Shrink action on the follower Sometimes throw in an extraneous unfollow just to check it does n't break anything Follow the index Make sure it actually took This should now be in the \\'warm\\ ' phase waiting for the index to be ready to unfollow Set the indexing_complete flag on the leader so the index will actually unfollow Wait for the setting to get replicated We ca n't reliably check that the index is unfollowed , because ILM moves through the unfollow and shrink actions so fast that the index often disappears between assertBusy checks Wait for the index to continue with its lifecycle and be shrunk Wait for the index to complete its policy Sometimes throw in an extraneous unfollow just to check it does n't break anything Move to a step from the injected unfollow action If we get an OK on this request we have successfully moved to the injected step Make sure we actually move on to and execute the shrink action"
      },
      "'1e8fb04b80aed5b907e2cf53fa2b45295f64573e'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "test the fields is not overwritten"
      },
      "'42ad9d43bc6bf13f0522574d6e5431efe1cb0e6d'": {
        "cm": "'conditional '",
        "comments": ""
      },
      "'d6fbfae1c569c69091430288253e75f5e952c3ba'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into cas_update_req '",
        "comments": "parse the logs and ensure that Elasticsearch died with the expected cause This test confirms JSON log structure is properly formatted and can be parsed . It has to be in a < code > org.elasticsearch.common.logging < /code > package to use < code > PrefixLogger < /code > message field will have a single line with json escaped stacktrace field will have each json line will in a separate array element need to use custom config path so we can use a custom log4j2.properties file for the test This test verifies that Elasticsearch can startup successfully with a custom logging config using variables introduced in < code > ESJsonLayout < /code > The intention is to confirm that users can still run their Elasticsearch instances with previous configurations . Formats log events as strings in a json format . These represent appenders and help docker distinguish log streams. < /li > < li > timestamp - ISO8601 with additional timezone ID < /li > < li > level - INFO , WARN etc < /li > < li > component - logger name , most of the times class name < /li > < li > cluster.name - taken from sys : es.logs.cluster_name system property because it is always set < /li > < li > node.name - taken from NodeNamePatternConverter , as it can be set in runtime as hostname when not set in elasticsearch.yml < /li > < li > node_and_cluster_id - in json as node.id and cluster.uuid - taken from NodeAndClusterIdConverter and present once clusterStateUpdate is first received < /li > < li > message - a json escaped message . Multiline messages will be converted to single line with new line explicitly replaced to \\ < li > exceptionAsJson - in json as a stacktrace field . Only present when throwable is passed as a parameter when using a logger . If first element is \\'short\\ ' , only the first line of the throwable will be formatted . Keeping those two fields together assures that they will be atomically set and become visible in logs at the same time . Called by log4j2 to initialize this converter . Updates only once the clusterID and nodeId . Once the first update is received , it will automatically be de-registered from subsequent updates . confirms exception is correctly parsed Represents a single log line in a json format . Parsing log lines with this class confirms the json format of logs Tests that extend this class verify that all json layout fields appear in the first few log lines after startup Fields available upon process startup : < code > type < /code > , < code > timestamp < /code > , < code > level < /code > , < code > component < /code > , < code > message < /code > , < code > node.name < /code > , < code > cluster.name < /code > . Whereas < code > node.id < /code > and < code > cluster.uuid < /code > are available later once the first clusterState has been received . < code > node.name < /code > , < code > cluster.name < /code > , < code > node.id < /code > , < code > cluster.uuid < /code > should not change across all log lines Note that this wo n't pass for nodes in clusters that do n't have the node name defined in elasticsearch.yml < strong > and < /strong > start with DEBUG or TRACE level logging . Those nodes log a few lines before the node.name is set by < code > LogConfigurator.setNodeName < /code > . Number of lines in the log file to check for the < code > node.name < /code > , < code > node.id < /code > or < code > cluster.uuid < /code > . We do n't just check the entire log file because it could be quite long The node name to expect in the log file . Open the log file . This is delegated to subclasses because the test framework does n't have permission to read from the log file but subclasses can grant themselves that permission . all lines should have the same nodeName and clusterName once the nodeId and clusterId are received , they should be the same on remaining lines Returns a stream of json log lines . This is intended to be used for easy and readable assertions for logger tests"
      }
    }
  },
  "elastic/elasticsearch_37821": {
    "id": "elastic/elasticsearch_37821",
    "body": "'Handling LLRC when * * live node size < 0 * * '",
    "cms": [
      "'Fix for #37739'",
      "'Adding Test Case'",
      "'Merge remote-tracking branch 'upstream/master''"
    ],
    "commits": {
      "'5c4ffc8a8cd880bfeefc8590f06e539c3ab1ff45'": {
        "cm": "",
        "comments": ""
      },
      "'b5625fd2bb786829266ae11ccf241fd2281c63ac'": {
        "cm": "'Adding Test Case '",
        "comments": "case when fewer nodeTuple than blacklist , wont result in any IllegalCapacityException"
      },
      "'ac497ff204e5240cdc8a961335cb0ac5edb7b2cc'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ''",
        "comments": "some test projects do n't have a main source set conventions are not honored when the tasks are disabled Creates an index using the Create Index API . Asynchronously creates an index using the Create Index API . A request to create an index . Constructs a new request to create an index with the specified name . The name of the index to create . The settings to create the index with . The settings to create the index with . The settings to create the index with . The settings to create the index with ( either json or yaml format ) Allows to set the settings using a json builder . The settings to create the index with ( either json/yaml/properties format ) Adds mapping that will be added when the index gets created . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the number of shard copies that should be active for index creation to return . Index creation will only wait up until the timeout value for the number of shard copies to be active before returning . Create index Create index with mappings , aliases and settings As Alias # equals only looks at name , we check the equality of the other Alias parameters here . Randomizes the index name , the aliases , mappings and settings associated with the index . When present , the mappings make no mention of types . Create a random server request , and copy its contents into the HLRC request . Because client requests only accept typeless mappings , we must swap out the mapping definition for one that does not contain types . Creates a random mapping , with no mention of types . reserve \\'null\\ ' for bwc . package private for tests validate the join on the joining node , will throw a failure if it fails the validation NodeToolCli does not extend LoggingAwareCommand , because LoggingAwareCommand performs logging initialization after LoggingAwareCommand instance is constructed . It 's too late for us , because before UnsafeBootstrapMasterCommand is added to the list of subcommands log4j2 initialization will happen , because it has static reference to Logger class . Even if we avoid making a static reference to Logger class , there is no nice way to avoid declaring UNSAFE_BOOTSTRAP , which depends on ClusterService , which in turn has static Logger . Returns null if the provided factory and his parents are compatible with this aggregator or the instance of the parent 's factory that is incompatible with the composite aggregation . different GeoPoints could map to the same or different hashing cells . The encoder to use to convert a geopoint 's ( lon , lat , precision ) into a long-encoded bucket key for aggregating . Aggregates data expressed as longs ( for efficiency 's sake ) but formats results as aggregation-specific strings . private impl that stores a bucket ord . This allows for computing the aggregations lazily . this is done because the aggregator may be rebuilt from cache ( non OrdinalBucket ) , or it may be rebuilding from a new calculation , and therefore copying bucketOrd . This method is used to return a re-usable instance of the bucket when building the aggregation . All geo-grid hash-encoding in a grid are of the same precision and held internally as a single long for efficiency 's sake . Read from a stream . package protected for testing Read from a stream . Returns the total hit count that should be tracked or null if the value is unset . Defaults to null . Returns true if this collector has early terminated . No node available for cluster Performs cluster bootstrap when node with id bootstrapNodeId is started . Any node of the batch could be selected as bootstrap target . client ( ) also starts the node build settings using same path.data as original but with node.data=false and node.master=false test that we can create data=false and master=false with no meta information test that we can create data=false env with only meta information . Also create shard data for following asserts assert that we get the stricter message on meta-data when both conditions fail build settings using same path.data as original but with node.master=false test that we can create master=false env regardless of data . test that we can create data=true , master=true env . Also remove state dir to leave only shard data for following asserts assert that we fail on shard data even without the metadata dir . ignore , this is an expected exception intentionally empty intentionally empty always track total hits accurately"
      }
    }
  },
  "elastic/elasticsearch_37797": {
    "id": "elastic/elasticsearch_37797",
    "body": "'We start using this class more often . Let 's make it a top-level class . '",
    "cms": [
      "'Make ChannelActionListener a top-level class\\n\\nWe start using this class more often. This commit makes it a top-level class.'",
      "'static logger'",
      "'Merge branch 'master' into channel-listener'"
    ],
    "commits": {
      "'c631cc6cb750a0434fddee123fba343c715a44ff'": {
        "cm": "'Make ChannelActionListener a top-level class\\n\\nWe start using this class more often . This commit makes it a top-level class . '",
        "comments": ""
      },
      "'a0afac5c9586d02ada5f1e515f899871acdfdb05'": {
        "cm": "'static logger '",
        "comments": ""
      },
      "'ef92abb3e8a764ddf2d1ea0bbe4f307733834dd3'": {
        "cm": "'Merge branch 'master ' into channel-listener '",
        "comments": "some test projects do n't have a main source set conventions are not honored when the tasks are disabled Creates an index using the Create Index API . Asynchronously creates an index using the Create Index API . Enable a native realm or built-in user synchronously . Constructs a new request to create an index with the specified name . The name of the index to create . The settings to create the index with . The settings to create the index with . The settings to create the index with . The settings to create the index with ( either json or yaml format ) Allows to set the settings using a json builder . The settings to create the index with ( either json/yaml/properties format ) Adds mapping that will be added when the index gets created . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the number of shard copies that should be active for index creation to return . Index creation will only wait up until the timeout value for the number of shard copies to be active before returning . Create index Create index with mappings , aliases and settings This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation 0 system roles plus the three we created As Alias # equals only looks at name , we check the equality of the other Alias parameters here . Randomizes the index name , the aliases , mappings and settings associated with the index . When present , the mappings make no mention of types . Create a random server request , and copy its contents into the HLRC request . Because client requests only accept typeless mappings , we must swap out the mapping definition for one that does not contain types . Creates a random mapping , with no mention of types . reserve \\'null\\ ' for bwc . The node-level ` discovery.zen.minimum_master_nodes ` setting on the master node that published this cluster state , for use in rolling upgrades from 6.x to 7.x . Once all the 6.x master-eligible nodes have left the cluster , the 7.x nodes use this value to determine how many master-eligible nodes must be discovered before the cluster can be bootstrapped . Note that this method returns the node-level value of this setting , and ignores any cluster-level override that was set via the API . Callers are expected to combine this value with any value set in the cluster-level settings . no Zen1 nodes found , but the last-known master was a Zen1 node , so this is a rolling upgrade NodeToolCli does not extend LoggingAwareCommand , because LoggingAwareCommand performs logging initialization after LoggingAwareCommand instance is constructed . It 's too late for us , because before UnsafeBootstrapMasterCommand is added to the list of subcommands log4j2 initialization will happen , because it has static reference to Logger class . Even if we avoid making a static reference to Logger class , there is no nice way to avoid declaring UNSAFE_BOOTSTRAP , which depends on ClusterService , which in turn has static Logger . Returns null if the provided factory and his parents are compatible with this aggregator or the instance of the parent 's factory that is incompatible with the composite aggregation . different GeoPoints could map to the same or different hashing cells . The encoder to use to convert a geopoint 's ( lon , lat , precision ) into a long-encoded bucket key for aggregating . Aggregates data expressed as longs ( for efficiency 's sake ) but formats results as aggregation-specific strings . private impl that stores a bucket ord . This allows for computing the aggregations lazily . this is done because the aggregator may be rebuilt from cache ( non OrdinalBucket ) , or it may be rebuilding from a new calculation , and therefore copying bucketOrd . This method is used to return a re-usable instance of the bucket when building the aggregation . All geo-grid hash-encoding in a grid are of the same precision and held internally as a single long for efficiency 's sake . Read from a stream . package protected for testing Read from a stream . Returns the total hit count that should be tracked or null if the value is unset . Defaults to null . Returns true if this collector has early terminated . Any node of the batch could be selected as bootstrap target . client ( ) also starts the node build settings using same path.data as original but with node.data=false and node.master=false test that we can create data=false and master=false with no meta information test that we can create data=false env with only meta information . Also create shard data for following asserts assert that we get the stricter message on meta-data when both conditions fail build settings using same path.data as original but with node.master=false test that we can create master=false env regardless of data . test that we can create data=true , master=true env . Also remove state dir to leave only shard data for following asserts assert that we fail on shard data even without the metadata dir ."
      }
    }
  },
  "elastic/elasticsearch_37788": {
    "id": "elastic/elasticsearch_37788",
    "body": "For the other contexts , the deprecation log must be used . '",
    "cms": [
      "'Add deprecation warning if TLSv1 used on transport'",
      "'Add warnings if TLSv1 is used implicitly\\n\\nTLSv1.0 will be removed from the default list of supported protocols\\nin v7.0.\\nThis change adds deprecation warnings when a TLS v1.0 connection is\\nused without having been explictly configured as a supported protocol.\\nSuch situations will fail in Elasticsearch 7.x'",
      "'Add tests for TLSv1 deprecation warnings'",
      "'Merge branch '6.x' into warn-use-tls10'",
      "'Add TLSv1 deprecation checks for LDAP connections'",
      "'Fix broken LDAP tests'",
      "'Add TLSv1 deprecation to SAML metadata loading'",
      "'Merge branch '6.x' into warn-use-tls10'"
    ],
    "commits": {
      "'deac7fa67f698358a6a075b3af8de0ab8dd3099a'": {
        "cm": "'Add deprecation warning if TLSv1 used on transport '",
        "comments": ""
      },
      "'e1e47261c1cdfaeb9376f7c21be7f1227c62bb9b'": {
        "cm": "'Add warnings if TLSv1 is used implicitly\\n\\nTLSv1.0 will be removed from the default list of supported protocols\\nin v7.0.\\nThis change adds deprecation warnings when a TLS v1.0 connection is\\nused without having been explictly configured as a supported protocol.\\nSuch situations will fail in Elasticsearch 7.x '",
        "comments": "This is handled here ( rather than as an interceptor ) so we know which profile was used , and can provide the correct setting to report on ( this may be technically also possible in a transport interceptor , but the existing security interceptor does n't have that now , and adding it would be a more intrusive change ) . Using the hostname verifier for this is just ugly , but HTTP client does n't expose the SSLSession in many places and this is the easiest one to hook into in a non-intrusive way Handles logging deprecation warnings when a TLSv1.0 SSL connection is used , and that SSL context relies on the default list of supported_protocols ( in Elasticsearch version , this list will not include TLS version ) . Use a \\'LRU\\ ' key that is unique per day . That way each description ( source address , etc ) will be logged once per day . This is handled here so that the deprecation warnings are returned to the HTTP client . If it were done in the network layer ( SecurityNetty4HttpServerTransport ) then the deprecation warning would be cleared from the thread context when the request was handled Set the description to include the remote host ( because it 's hard to fix clients if you do n't know who they are But do n't include the port because then every connection would have a different deprecation key"
      },
      "'cfd674e8e6d05d692080325b73213f9d94b81fbb'": {
        "cm": "'Add tests for TLSv1 deprecation warnings '",
        "comments": ""
      },
      "'85269f725cbce1e5b0a1828ea3e9be5f658322bd'": {
        "cm": "'Merge branch ' 6.x ' into warn-use-tls10 '",
        "comments": "we need to specify the exact version of build-tools because gradle automatically adds its plugin portal which appears to mirror jcenter , opening us up to pulling a \\'later\\ ' version of build-tools Updates the mappings on an index using the Put Mapping API . Asynchronously updates the mappings on an index using the Put Mapping API . The higher the priority , the faster the recovery . package private for testing Put a mapping definition into one or more indices . If an index already contains mappings , the new mappings will be merged with the existing one . If there are elements that can not be merged , the request will be rejected . Constructs a new put mapping request against one or more indices . If no indices are provided then it will be executed against all indices . The indices into which the mappings will be put . The mapping source definition . The mapping source definition . Note that the definition should * not * be nested under a type name . The mapping source definition . Note that the definition should * not * be nested under a type name . The mapping source definition . Note that the definition should * not * be nested under a type name . The mapping source definition . Note that the definition should * not * be nested under a type name . True if the privileges cover restricted internal indices too . Certain indices are reserved for internal services and should be transparent to ordinary users . For that matter , when granting privileges , you also have to toggle this flag to confirm that all indices , including restricted ones , are in the scope of this permission . By default this is false . we convert all the method names to snake case , hence we need to look for the '_async ' suffix rather than 'Async ' TODO xpack api are currently ignored , we need to load xpack yaml spec too Create leader index : Follow index , so that we can query for follow stats : if the original ` source ` is null , the parsed source should be so too This sub object should support unknown fields , but metadata can not contain complex extra objects or it will fail This may have extra json , so lets just assume that if all of the original fields from the creation are there , then its equal This is the same code that is in createTestInstance in this class . Accessing the future will yield the exception fixed_auto_queue_size wraps stuff into TimedRunnable , which is an AbstractRunnable TODO : this is dangerous as it will silently swallow exceptions , and possibly miss calling a response listener scheduler just swallows the exception here TODO : bubble these exceptions up fixed_auto_queue_size wraps stuff into TimedRunnable , which is an AbstractRunnable TODO : this is dangerous as it will silently swallow exceptions , and possibly miss calling a response listener scheduler just swallows the exceptions TODO : bubble these exceptions up SnapshotInProgressException was introduced after CoordinationStateRejectedException ( id = 0 ) , which was not backported from version . Uses the class-local logger . Going to be removed in subsequent versions . The parameterless constructor should be used instead . Reads a list of strings . So we walk the string looking for either of the characters that we need to escape . The rewrite throws an error if more than < code > maxExpansions < /code > terms are found and < code > hardLimit < /code > is set . field does not exist dates starting with 0 will not be using joda but java time formatters A rate limiter designed for multiple concurrent users . TODO : This rate limiter has some concurrency issues between the two maybePause operations Time to pause Checks if the runnable arose from asynchronous submission of a task to an executor . If an uncaught exception was thrown during the execution of this task , we need to inspect this runnable and see if it is an error that should be propagated to the uncaught exception handler . In theory , Future # get can only throw a cancellation exception , an interrupted exception , or an execution exception . We want to ignore cancellation exceptions , restore the interrupt status on interrupted exceptions , and inspect the cause of an execution . We are going to be extra paranoid here though and completely unwrap the exception to ensure that there is not a buried error anywhere . We assume that a general exception has been handled by the executed task or the task submitter . throw this error where it will propagate to the uncaught exception handler restore the interrupt status Updating the thread local is expensive due to a shared reference that we synchronize on , so we should only do it if the thread context struct changed . It will not change if we de-duplicate this value to an existing one , or if we do n't add a new one because we have reached capacity . Build an appropriate query based on the analysis chain . phase 0 : read through the stream and assess the situation : counting the number of tokens/positions and marking if we have any synonyms . phase 0 : based on token count , presence of synonyms , and options formulate a single term , boolean , or phrase . single term graph phrase complex phrase with synonyms simple phrase phrase prefix boolean only one position , with synonyms complex case : multiple positions Use the analyzer to get all the tokens , and then build an appropriate query based on the analysis chain . Creates a boolean query from the graph token stream by extracting all the finite strings from the graph and using them to create phrase queries with the appropriate slop . Creates a span near ( phrase ) query from a graph token stream . The articulation points of the graph are visited in order and the queries created at each point are merged in the returned near query . ignore unmapped fields We have to pick some field to pass through the superclass so we just pick the first field . It should n't matter because fields are already grouped by their analyzers/types . apply the field boost to groups that contain a single field For a sequence based recovery , the target can keep its local translog add shard to replication group ( shard will receive replication requests from this point on ) now that engine is open . This means that any document indexed into the primary after this will be replicated to this replica as well make sure to do this before sampling the max sequence number in the next step , to ensure that we send all documents up to maxSeqNo in phase2 . We need to wait for all operations up to the current max to complete , otherwise we can not guarantee that all operations in the required range will be available for replaying from the translog of the source . We need to synchronized Snapshot # next ( ) because it 's called by different threads through sendBatch . Even though those calls are not concurrent , Snapshot # next ( ) uses non-synchronized state and is not multi-thread-compatible . check if this request is past bytes threshold , and if so , send it off send the leftover operations or if no operations were sent , request the target to respond with its local checkpoint The maxSeenAutoIdTimestampOnPrimary received from the primary is at least the highest auto_id_timestamp from any operation will be replayed . Bootstrapping this timestamp here will disable the optimization for original append-only requests ( source of these operations ) replicated via replication . Without this step , we may have duplicate documents if we replay these operations first ( without timestamp ) , then optimize append-only requests ( with timestamp ) . Bootstrap the max_seq_no_of_updates from the primary to make sure that the max_seq_no_of_updates on this replica when replaying any of these operations will be at least the max_seq_no_of_updates on the primary when that op was executed on . update stats only after all operations completed ( to ensure that mapping updates do n't mess with stats ) roll over / flush / trim if needed before version we responded with an empty response so we have to maintain that This unassigns a task from any node , i.e . It is primarily designed to be used to restore from some form of a snapshot . It will setup a new store , identify files that need to be copied for the source , and perform the copies . Implementers must implement the functionality of opening the underlying file streams for snapshotted lucene file . If we are restoring a snapshot created by a previous supported version , it is still possible that in that version , an empty shard has a segments_N file with an unsupported version ( and no checksum ) , because we do n't know the Lucene version to assign segments_N until we have written some data . Since the segments_N for an empty shard could have an incompatible Lucene version number and no checksum , even though the index itself is perfectly fine to restore , this empty shard would cause exceptions to be thrown . Since there is no data to restore from an empty shard anyway , we just create the empty shard here and then exit . this will throw an IOException if the store has no segments infos file . The store can still have existing files but they will be deleted just before being restored . happens when restore to an empty shard , not a big deal in version we added additional hashes for .si / segments_N files to ensure we do n't double the space in the repo since old snapshots do n't have this hash we try to read that hash from the blob store in a bwc compatible way . if the index is broken we might not be able to read it list of all existing store files restore the files from the snapshot to the Lucene store if a file with a same physical name already exist in the store we need to delete it before restoring it from the snapshot . We could be lenient and try to reuse the existing store files ( and compare their names/length/checksum again with the snapshot files ) but to avoid extra complexity we simply delete them and restore them again like StoreRecovery does with dangling indices . Any existing store file that is not restored from the snapshot will be clean up by RecoveryTarget.cleanFiles ( ) . we have a hash - check if our repo has a hash too otherwise we have to calculate it . we might have multiple parts even though the file is small ... make sure we read all of it . Thrown on the attempt to execute an action that requires that no snapshot is in progress . This subclass ensures to properly bubble up Throwable instances of type Error . Returns the preferred discovery node for this request . The remote cluster client will attempt to send this request directly to this node . Otherwise , it will send the request as a proxy action that will be routed by the remote cluster to this node . tasks that are scheduled for the future . Runs an arbitrary runnable task . Schedule a task for immediate execution . Schedule a task for future execution . Advance the current time to the time of the next deferred task , and update the sets of deferred and runnable tasks accordingly . Creates a random mapping , with no mention of types . Creates a random mapping , with the mapping definition nested under the given type name . Creates a new empty shard and starts it . Dynamic node setting for specifying the wait_for_timeout that the auto follow coordinator and shard follow task should be using . Max bytes a node can recover per second . must capture after snapshotting operations to ensure this MUS is at least the highest MUS of any of these operations . must capture IndexMetaData after snapshotting operations to ensure the returned mapping version is at least as up-to-date as the mapping version that these operations used . Here we must not use IndexMetaData from ClusterService for we expose a new cluster state to ClusterApplier ( s ) before exposing it in the ClusterService . Only retry is the shard follow task is not stopped . if wait_for_metadata_version timeout , the response is empty ask for the next version . TODO : Currently blocking actions might occur in the session closed callbacks . This dispatch may be unnecessary when we remove these callbacks . This is currently safe to do because calling ` onResponse ` will serialize the bytes to the network layer data structure on the same thread . So the bytes will be copied before the reference is released . TODO : There should be some local timeout . And if the remote cluster returns an unknown session response , we should be able to retry by creating a new session . Read bytes into the reference from the file . This method will return the offset in the file where the read completed . Previously we would throw a NullPointerException when there was no persistent tasks metadata in the cluster state . This tests maintains that we do not make this mistake again . This makes the ClusterService have an older mapping version than the actual mapping version that IndexService will use to index \\'doc1\\ ' . Make sure at least one read-request which requires mapping sync is completed . Cancel just before attempting to fetch operations : Would throw exception if missing Would throw exception if missing Would throw exception if missing The tasks will not be rescheduled as the sessions are closed . Would throw exception if missing Would throw exception if missing Request a second file to ensure that original file is not leaked Exception will be thrown if file is not closed . Session starts as not idle . First task will mark it as idle Task is still scheduled Accessing session marks it as not-idle Check session exists Task is still scheduled Task is cancelled when the session times out This is an abstract AsyncActionStep that wraps the performed action listener , checking to see if the action fails due to a snapshot being in progress . If a snapshot is in progress , it registers an observer and waits to try again when a snapshot is no longer running . Wrap the original listener to handle exceptions caused by ongoing snapshots Method to be performed during which no snapshots for the index are already underway . The index has since been deleted , mission accomplished ! Re-invoke the performAction method with the new state TODO : what is a good timeout value for no new state received during this time ? If a snapshot is still running it registers a new listener and tries again . Passes any exceptions to the original exception listener if they occur . The higher the priority , the faster the recovery . package private for testing Converts a CCR following index into a normal , standalone index , once the index is ready to be safely separated . 0 ) The index must be up to date with the leader , defined as the follower checkpoint being equal to the global checkpoint for all shards . There are no settings to change , so therefor this action should be safe :"
      },
      "'412cdefa78a072144776670cce0b0293e76c45da'": {
        "cm": "'Add TLSv1 deprecation checks for LDAP connections '",
        "comments": ""
      },
      "'e324de5cfed38c678699092839d2ac5ad76d51dd'": {
        "cm": "'Fix broken LDAP tests '",
        "comments": ""
      },
      "'7b6cbe32b130c3856399f7442be999f811016384'": {
        "cm": "'Add TLSv1 deprecation to SAML metadata loading '",
        "comments": ""
      },
      "'db004169c22d60dc624526f3c81dfe6da9d8b3f5'": {
        "cm": "'Merge branch ' 6.x ' into warn-use-tls10 '",
        "comments": "some test projects do n't have a main source set conventions are not honored when the tasks are disabled Creates an index using the Create Index API . Asynchronously creates an index using the Create Index API . Retrieves the field mappings on an index or indices using the Get Field Mapping API . Constructs a new request to create an index with the specified name . The name of the index to create . The settings to create the index with . The settings to create the index with . The settings to create the index with . The settings to create the index with ( either json or yaml format ) Allows to set the settings using a json builder . The settings to create the index with ( either json/yaml/properties format ) Adds mapping that will be added when the index gets created . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the number of shard copies that should be active for index creation to return . Index creation will only wait up until the timeout value for the number of shard copies to be active before returning . Indicate whether the receiving node should operate based on local index information or forward requests , where needed , to other nodes . If running locally , request will not raise errors if running locally & amp ; missing indices . Returns the fields mapping . The return map keys are indexes and fields ( as specified in the request ) . Returns the mappings of a specific index and field . Returns the mappings as a map . pkg-private for testing Create index Create index with mappings , aliases and settings This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation 0 system roles plus the three we created As Alias # equals only looks at name , we check the equality of the other Alias parameters here . Randomizes the index name , the aliases , mappings and settings associated with the index . When present , the mappings make no mention of types . Create a random server request , and copy its contents into the HLRC request . Because client requests only accept typeless mappings , we must swap out the mapping definition for one that does not contain types . Creates a random mapping , with no mention of types . metadata is a series of kv pairs , so we dont want to add random fields here for test equality This class represents a trust configuration that corresponds to the default trusted CAs of the JDK Create a trust config that uses System properties to determine the TrustStore type , and the relevant password . JKS keystores where we only need to pass the password for reading Private Key entries ) . Can only support length encoding up to 0 octets . < p > In BER/DER encoding , length can be encoded in 0 forms : < /p > < ul > < li > Short form . One octet . Bit 0 has value \\ ' 0\\ ' and bits 70 give the length . < /li > < li > Long form . Two to 0 octets ( only 0 is supported here ) . Bit 0 of first octet has value \\ ' 1\\ ' and bits 70 give the number of additional length octets . Second and following octets give the length , base 0 , most significant digit first . The object is not parsed . It can only handle integers . The TLV could be either a constructed or primitive entity . < p > The first byte in DER encoding is made of following fields : < /p > < pre > -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - |Bit 8|Bit 7|Bit 6|Bit 5|Bit 4|Bit 3|Bit 2|Bit 1| -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | Class | CF | + Type | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - < /pre > < ul > < li > Class : Universal , Application , Context or Private < li > CF : Constructed flag . If 0 , the field is constructed . < li > Type : This is actually called tag in ASN.1 . It indicates data type ( Integer , String ) or a construct ( sequence , choice , set ) . For constructed field , return a parser for its content . This method only references the < em > file name < /em > of the keystore , it does not look at its contents . Read the given keystore file . Construct a new trust config for the provided paths . RFC 0 indicates that supported algorithms are defined in RFC 0 . RFC 0 only defines DES-CBS and triple DES ( EDE ) in CBC mode . Never request a client certificate . nothing to do here Request a client certificate , but do not enforce that one is provided . Request and require a client certificate . A object encapsulating all necessary configuration for an SSL context ( client or server ) . It is recommended that these files be monitored for changes , and a new ssl-context is created whenever any of the files are modified . Dynamically create a new SSL context based on the current state of the configuration . Utility class for handling the standard setting keys for use in SSL configuration . If no key password is specified , it will default to the keystore password . If no key password is specified , it will default to the keystore password . The list of keys that are used to load a non-secure , non-list setting The list of keys that are used to load a non-secure , list setting The list of keys that are used to load a secure setting ( such as a password ) that would typically be stored in the elasticsearch keystore . This class handles the logic of interpreting the various \\'ssl . * \\ ' configuration settings and their interactions ( as well as being aware of dependencies and conflicts between different settings ) . It may be the empty string , otherwise it must end in a \\'.\\ ' ( period ) . Change the default trust config . Change the default client authentication mode . Change the default supported ciphers . < p > The setting should be returned as a string , and this class will convert it to the relevant type . Clients of this class should implement this method to load a fully-qualified key from the preferred secure settings source . Clients of this class should implement this method to load a fully-qualified key from the preferred settings source . < p > The setting should be returned as a list of strings , and this class will convert the values to the relevant type . Typically points to the Elasticsearch configuration directory . No AES ? Things are going to be very weird , but technically that means we do n't have 0 bit AES , so ... An interface for building a key manager at runtime . The method for constructing the key manager is implementation dependent . An interface for building a trust manager at runtime . The method for constructing the trust manager is implementation dependent . Represents the verification mode to be used for SSL connections . Verify neither the hostname , nor the provided certificate . Verify the provided certificate against the trust chain , but do not verify the hostname . Verify the provided certificate against the trust chain , and also verify that the hostname to which this client is connected matches one of the Subject-Alternative-Names in the certificate . Verifies that the keystore contains at least 0 private key entry . Verifies that the keystore contains at least 0 trusted certificate entry . This class can not be used on FIPS0 JVM as it has its own trust manager implementation . All methods are implemented as a no-op and do not throw exceptions regardless of the certificate presented . This is a sample of the CAs that we expect on every JRE . We can safely change this list if the JRE 's issuer list changes , but we want to assert something useful . When running on BC-FIPS , an invalid file format behaves like an empty file A test for non-trust , non-key configurations . These are straight forward and can all be tested together If this is not set , the loader will guess from the extension If this is not set , the loader will guess from the extension If this is not set , the loader will guess from the extension If this is not set , the loader will guess from the extension Because ( a ) can not load a JKS as a PKCS12 & ( b ) the password is wrong . in case you are wondering why we do not call 'DateFormatter.forPattern ( format ) ' for all cases here , but only for the non java time case : When the joda date formatter parses a date then a year is always set , so that no fallback can be used , like done in the JodaDateFormatter.withYear ( ) code below This means that we leave the existing parsing logic in place , but will fall back to the new java date parsing logic , if an \\ ' 8\\ ' is prepended to the date format string Parse the user agent in the ECS ( Elastic Common Schema ) format Deprecated format , removed in version Deprecated in version ( superceded by VERSION ) , to be removed in version Deprecated in version ( superceded by just using OS ) , to be removed in version Request the mappings of specific fields Note : there is a new class with the same name for the Java HLRC that uses a typeless format . Any changes done to this class should go to that client class as well . Any changes done to this class should go to that client class as well . reserve \\'null\\ ' for bwc . The node-level ` discovery.zen.minimum_master_nodes ` setting on the master node that published this cluster state , for use in rolling upgrades from 6.x to 7.x . Once all the 6.x master-eligible nodes have left the cluster , the 7.x nodes use this value to determine how many master-eligible nodes must be discovered before the cluster can be bootstrapped . Note that this method returns the node-level value of this setting , and ignores any cluster-level override that was set via the API . Callers are expected to combine this value with any value set in the cluster-level settings . use a linked hash set to preserve order preserve insertion order A callback when a new retention lease is created or an existing retention lease expires . In practice , this callback invokes the retention lease sync action , to sync retention leases to replicas . the primary calculates the non-expired retention leases and syncs them to replicas early out as no retention leases have expired clean up the expired retention leases At this point , we were either in primary mode and have updated the non-expired retention leases into the tracking map , or we were in replica mode and merely need to copy the existing retention leases since a replica does not calculate the non-expired retention leases , instead receiving them on syncs from the primary . Renews an existing retention lease . This action is deliberately a write action so that if a replica misses a retention lease sync then that shard will be marked as stale . Sync the specified retention leases for the specified shard . The callback is invoked when the sync succeeds or fails . Represents a method that when invoked syncs retention leases to replica shards after a new retention lease is added on the primary . The specified listener is invoked when the syncing completes with success or failure . An unmodifiable copy of the retention leases is returned ."
      }
    }
  },
  "elastic/elasticsearch_37772": {
    "id": "elastic/elasticsearch_37772",
    "body": "'Some of our newer endpoints and indices were missing from\\r\\nthe tests . '",
    "cms": [
      "'[ML] Fix gaps in reserved roles tests\\n\\nSome of our newer endpoints and indices were missing from\\nthe tests.'",
      "'Merge branch 'master' into fix_gaps_in_roles_tests'"
    ],
    "commits": {
      "'ba46f4125150b915bfbc0e5e0f8a6ecb967b456e'": {
        "cm": "' [ ML ] Fix gaps in reserved roles tests\\n\\nSome of our newer endpoints and indices were missing from\\nthe tests . '",
        "comments": ""
      },
      "'f8198e6cc04f567d770dd06d8d6f319e2fe46ec1'": {
        "cm": "'Merge branch 'master ' into fix_gaps_in_roles_tests '",
        "comments": "some test projects do n't have a main source set conventions are not honored when the tasks are disabled we need to specify the exact version of build-tools because gradle automatically adds its plugin portal which appears to mirror jcenter , opening us up to pulling a \\'later\\ ' version of build-tools Retrieves the field mappings on an index or indices using the Get Field Mapping API . If running locally , request will not raise errors if running locally & amp ; missing indices . Returns the fields mapping . The return map keys are indexes and fields ( as specified in the request ) . Returns the mappings of a specific index and field . Returns the mappings as a map . pkg-private for testing This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation 0 system roles plus the three we created allow random fields at the level of ` index ` and ` index.mappings.field ` otherwise random field could be evaluated as index name or type name if mappings is empty , means that fields are not found As the client class GetFieldMappingsResponse does n't have toXContent method , adding this method here only for the test metadata is a series of kv pairs , so we dont want to add random fields here for test equality Request the mappings of specific fields Note : there is a new class with the same name for the Java HLRC that uses a typeless format . Any changes done to this class should go to that client class as well . Any changes done to this class should go to that client class as well . reserve \\'null\\ ' for bwc . The node-level ` discovery.zen.minimum_master_nodes ` setting on the master node that published this cluster state , for use in rolling upgrades from 6.x to 7.x . Once all the 6.x master-eligible nodes have left the cluster , the 7.x nodes use this value to determine how many master-eligible nodes must be discovered before the cluster can be bootstrapped . Note that this method returns the node-level value of this setting , and ignores any cluster-level override that was set via the API . Callers are expected to combine this value with any value set in the cluster-level settings . no Zen1 nodes found , but the last-known master was a Zen1 node , so this is a rolling upgrade It 's possible for replicaNodeVersion to be null , when disassociating dead nodes nothing to do until we actually recover from the gateway or any other block indicates we need to disable persistency Reads a list of strings . The consumer is only notified if at least one of the settings change . < /p > use a linked hash set to preserve order preserve insertion order This unassigns a task from any node , i.e . The ack timeout of 0 on dynamic mapping updates makes it possible for the document to be indexed on the primary , even if the dynamic mapping update is not applied on the replica yet . ... and wait for second mapping to be available on master Speed up rechecks to a rate that is quicker than what settings would allow Disallow re-assignment after it is unallocated to verify master and node state Verify that the task is NOT running on the node Verify that the task is STILL in internal cluster state Allow it to be reassigned again to the same node Verify it starts again Complete or cancel the running task Wait for the task to start ensures we do n't block Dynamic node setting for specifying the wait_for_timeout that the auto follow coordinator should be using . TODO : Deprecate and remove this setting must capture after snapshotting operations to ensure this MUS is at least the highest MUS of any of these operations . must capture IndexMetaData after snapshotting operations to ensure the returned mapping version is at least as up-to-date as the mapping version that these operations used . Here we must not use IndexMetaData from ClusterService for we expose a new cluster state to ClusterApplier ( s ) before exposing it in the ClusterService . if wait_for_metadata_version timeout , the response is empty ask for the next version . Pause follower1 index and check the follower info api : Block the ClusterService from exposing the cluster state with the mapping change . This makes the ClusterService have an older mapping version than the actual mapping version that IndexService will use to index \\'doc1\\ ' . Make sure at least one read-request which requires mapping sync is completed . This is an abstract AsyncActionStep that wraps the performed action listener , checking to see if the action fails due to a snapshot being in progress . If a snapshot is in progress , it registers an observer and waits to try again when a snapshot is no longer running . Wrap the original listener to handle exceptions caused by ongoing snapshots Method to be performed during which no snapshots for the index are already underway . The index has since been deleted , mission accomplished ! Re-invoke the performAction method with the new state TODO : what is a good timeout value for no new state received during this time ? If a snapshot is still running it registers a new listener and tries again . Passes any exceptions to the original exception listener if they occur . No snapshots are running , new state is acceptable to proceed There is a snapshot running with this index name There are snapshots , but none for this index , so it 's okay to proceed with this state This means the cluster is being shut down , so nothing to do here A utility class used for loading index lifecycle policies from the resource classpath Loads a built-in index lifecycle policy and returns its source . Create the repository before taking the snapshot . start snapshot add policy and expect it to trigger unfollow immediately ( while snapshot in progress ) Ensure that 'index.lifecycle.indexing_complete ' is replicated : ILM should have unfollowed the follower index , so the following_index setting should have been removed : ( this controls whether the follow engine is used ) Following index should have the document ILM should have completed the unfollow assert that snapshot succeeded Create the repository before taking the snapshot . create delete policy create index without policy index document so snapshot actually does something start snapshot add policy and expect it to trigger delete immediately ( while snapshot in progress ) assert that index was deleted assert that snapshot is still in progress and clean up Create the repository before taking the snapshot . create delete policy create index without policy required so the shrink does n't wait on SetSingleNodeAllocateStep index document so snapshot actually does something start snapshot add policy and expect it to trigger shrink immediately ( while snapshot in progress ) assert that index was shrunk and original index was deleted assert that snapshot succeeded Create the repository before taking the snapshot . create delete policy create index without policy index document so snapshot actually does something start snapshot add policy and expect it to trigger delete immediately ( while snapshot in progress ) assert that the index froze assert that snapshot is still in progress and clean up audit trail service construction .security index is not managed by using templates anymore the only available output type is \\'logfile\\ ' , but the optputs= < list > is to keep compatibility with previous reporting format When the histogram in SQL is applied on DATE type instead of DATETIME , the interval specified is truncated to the multiple of a day . If the interval specified is less than 0 day , then the interval used will be ` INTERVAL ' 0 ' DAY ` . For testing"
      }
    }
  },
  "elastic/elasticsearch_37769": {
    "id": "elastic/elasticsearch_37769",
    "body": "'Made the test tolerant to index upgrade being run\\r\\nin between the old/mixed/upgraded portions .",
    "cms": [
      "'[TEST] Fix MlMappingsUpgradeIT testMappingsUpgrade\\n\\nMade the test tolerant to index upgrade being run\\nin between the old/mixed/upgraded portions.  This\\ncan occur because the rolling upgrade tests all\\nshare the same indices.\\n\\nFixes #37763'",
      "'Merge branch 'master' into fix_mapping_upgrade_test'"
    ],
    "commits": {
      "'6686323c2d80e92f86f81ce22c7c30f4440aa29c'": {
        "cm": "' [ TEST ] Fix MlMappingsUpgradeIT testMappingsUpgrade\\n\\nMade the test tolerant to index upgrade being run\\nin between the old/mixed/upgraded portions .",
        "comments": "Use a custom index because other rolling upgrade tests meddle with the shared index The name of the concrete index underlying the results index alias may or may not have been changed by the upgrade process ( depending on what other tests are being run and the order they 're run in ) , so navigating to the next level of the tree must account for both cases"
      },
      "'eb31a5c955cb15f57ef5c3e59005b6840a769e44'": {
        "cm": "'Merge branch 'master ' into fix_mapping_upgrade_test '",
        "comments": "some test projects do n't have a main source set conventions are not honored when the tasks are disabled we need to specify the exact version of build-tools because gradle automatically adds its plugin portal which appears to mirror jcenter , opening us up to pulling a \\'later\\ ' version of build-tools Creates an index using the Create Index API . Asynchronously creates an index using the Create Index API . Retrieves the field mappings on an index or indices using the Get Field Mapping API . Constructs a new request to create an index with the specified name . The name of the index to create . The settings to create the index with . The settings to create the index with . The settings to create the index with . The settings to create the index with ( either json or yaml format ) Allows to set the settings using a json builder . The settings to create the index with ( either json/yaml/properties format ) Adds mapping that will be added when the index gets created . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the definition should * not * be nested under a type name . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the settings and mappings as a single source . Note that the mapping definition should * not * be nested under a type name . Sets the number of shard copies that should be active for index creation to return . Index creation will only wait up until the timeout value for the number of shard copies to be active before returning . Indicate whether the receiving node should operate based on local index information or forward requests , where needed , to other nodes . If running locally , request will not raise errors if running locally & amp ; missing indices . Returns the fields mapping . The return map keys are indexes and fields ( as specified in the request ) . Returns the mappings of a specific index and field . Returns the mappings as a map . pkg-private for testing Create index Create index with mappings , aliases and settings This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation This is no longer true for all methods . Some methods can contain these 0 args backwards because of deprecation 0 system roles plus the three we created As Alias # equals only looks at name , we check the equality of the other Alias parameters here . Randomizes the index name , the aliases , mappings and settings associated with the index . When present , the mappings make no mention of types . Create a random server request , and copy its contents into the HLRC request . Because client requests only accept typeless mappings , we must swap out the mapping definition for one that does not contain types . Creates a random mapping , with no mention of types . metadata is a series of kv pairs , so we dont want to add random fields here for test equality case when fewer nodeTuple than blacklist , wont result in any IllegalCapacityException in case you are wondering why we do not call 'DateFormatter.forPattern ( format ) ' for all cases here , but only for the non java time case : When the joda date formatter parses a date then a year is always set , so that no fallback can be used , like done in the JodaDateFormatter.withYear ( ) code below This means that we leave the existing parsing logic in place , but will fall back to the new java date parsing logic , if an \\ ' 8\\ ' is prepended to the date format string Request the mappings of specific fields Note : there is a new class with the same name for the Java HLRC that uses a typeless format . Any changes done to this class should go to that client class as well . Any changes done to this class should go to that client class as well . reserve \\'null\\ ' for bwc . The node-level ` discovery.zen.minimum_master_nodes ` setting on the master node that published this cluster state , for use in rolling upgrades from 6.x to 7.x . Once all the 6.x master-eligible nodes have left the cluster , the 7.x nodes use this value to determine how many master-eligible nodes must be discovered before the cluster can be bootstrapped . Note that this method returns the node-level value of this setting , and ignores any cluster-level override that was set via the API . Callers are expected to combine this value with any value set in the cluster-level settings . no Zen1 nodes found , but the last-known master was a Zen1 node , so this is a rolling upgrade NodeToolCli does not extend LoggingAwareCommand , because LoggingAwareCommand performs logging initialization after LoggingAwareCommand instance is constructed . It 's too late for us , because before UnsafeBootstrapMasterCommand is added to the list of subcommands log4j2 initialization will happen , because it has static reference to Logger class . Even if we avoid making a static reference to Logger class , there is no nice way to avoid declaring UNSAFE_BOOTSTRAP , which depends on ClusterService , which in turn has static Logger . Reads a list of strings . Adds a affix settings consumer that accepts the settings for a group of settings . The consumer is only notified if at least one of the settings change . < /p > use a linked hash set to preserve order preserve insertion order Flatten multipoints write a snapshot of current time , which is not per se the time field This unassigns a task from any node , i.e . Returns null if the provided factory and his parents are compatible with this aggregator or the instance of the parent 's factory that is incompatible with the composite aggregation . different GeoPoints could map to the same or different hashing cells . The encoder to use to convert a geopoint 's ( lon , lat , precision ) into a long-encoded bucket key for aggregating . Aggregates data expressed as longs ( for efficiency 's sake ) but formats results as aggregation-specific strings . private impl that stores a bucket ord . This allows for computing the aggregations lazily . this is done because the aggregator may be rebuilt from cache ( non OrdinalBucket ) , or it may be rebuilding from a new calculation , and therefore copying bucketOrd . This method is used to return a re-usable instance of the bucket when building the aggregation . All geo-grid hash-encoding in a grid are of the same precision and held internally as a single long for efficiency 's sake . Read from a stream . package protected for testing Read from a stream . Returns the total hit count that should be tracked or null if the value is unset . Defaults to null . Returns true if this collector has early terminated . Any node of the batch could be selected as bootstrap target ."
      }
    }
  },
  "elastic/elasticsearch_37713": {
    "id": "elastic/elasticsearch_37713",
    "body": "'AuthenticateResponse did not allow unknown fields .",
    "cms": [
      "'Update authenticate to allow unknown fields\\n\\nAuthenticateResponse did not allow unknown fields. This commit fixes the\\ntest and ConstructingObjectParser such that it does now allow unknown\\nfields.\\n\\nRelates #36938'",
      "'Merge remote-tracking branch 'upstream/master' into rest_cop_security_authenticate'"
    ],
    "commits": {
      "'8d1e5a7ea9db3a2f2dca0e93ad7e4024e8837faa'": {
        "cm": "'Update authenticate to allow unknown fields\\n\\nAuthenticateResponse did not allow unknown fields .",
        "comments": "metadata is a series of kv pairs , so we dont want to add random fields here for test equality"
      },
      "'a45effbf04fe49cdd66d65aa9fe996c1215a40a9'": {
        "cm": "'Merge remote-tracking branch 'upstream/master ' into rest_cop_security_authenticate '",
        "comments": "we need to specify the exact version of build-tools because gradle automatically adds its plugin portal which appears to mirror jcenter , opening us up to pulling a \\'later\\ ' version of build-tools Retrieves the field mappings on an index or indices using the Get Field Mapping API . If running locally , request will not raise errors if running locally & amp ; missing indices . Returns the fields mapping . The return map keys are indexes and fields ( as specified in the request ) . Returns the mappings of a specific index and field . Returns the mappings as a map . Any changes done to this class should go to that client class as well . Any changes done to this class should go to that client class as well . Reads a list of strings . Adds a affix settings consumer that accepts the settings for a group of settings . The consumer is only notified if at least one of the settings change . < /p > support the 6.x BWC compatible way of parsing java 0 dates this one here is lenient as well to retain joda time based bwc compatibility if there is already a milli of second , we need to overwrite it the third formatter fails under java 0 as a printer , so fall back to this one TODO only millisecond granularity here ! this just ensure that the pattern is actually valid , no need to keep it here This unassigns a task from any node , i.e . AggregationInspectionHelper delegates to these helpers when needed , and consumers should prefer to use AggregationInspectionHelper instead of these helpers . TODO better way to know if the scripted metric received documents ? Provides a set of static helpers to determine if a particular type of InternalAggregation \\'has a value\\ ' or not . This can be difficult to determine from an external perspective because each agg uses different internal bookkeeping to determine if it is empty or not ( NaN , +/-Inf , version , etc ) . This set of helpers aim to ease that task by codifying what \\'empty\\ ' is for each agg . It is not entirely accurate for all aggs , since some do not expose or track the needed state ( e.g . sum does n't record count , so it 's not clear if the sum is version because it is empty or because of summing to zero ) . Pipeline aggs in particular are not well supported by these helpers since most share InternalSimpleValue and it 's not clear which pipeline generated the value . TODO this could be incorrect ... e.g . we have to check the primary term field as it is only assigned for non-nested documents Returns whether this cluster is configured to be skipped when unavailable Returns whether the cluster identified by the provided alias is configured to be skipped when unavailable setting skip_unavailable to true for all the disconnected clusters will make the request succeed again give transport service enough time to realize that the node is down , and to notify the connection listeners so that RemoteClusterConnection is left with no connected nodes , hence it will retry connecting next if it 's the first master in the cluster bootstrap the cluster with this node name initial configuration should not have a place holder for local node this test requires tests to run with -Djava.locale.providers=COMPAT in order to work public void testCustomTimeFormats ( ) { assertSameDate ( \\'2010 0 0 11:05:15\\ ' , \\'yyyy dd MM HH : mm : ss\\ ' ) ; assertSameDate ( \\'12/06\\ ' , \\'dd/MM\\ ' ) ; assertSameDate ( \\'Nov 0 01:29:01 -0800\\ ' , \\'MMM dd HH : mm : ss Z\\ ' ) ; // also ensure that locale based dates are the same assertSameDate ( \\'Di. , 0 Dez . 0 02:55:00 -0800\\ ' , \\ ' E , d MMM yyyy HH : mm : ss Z\\ ' , LocaleUtils.parse ( \\'de\\ ' ) ) ; assertSameDate ( \\'Mi. , 0 Dez . 0 02:55:00 -0800\\ ' , \\ ' E , d MMM yyyy HH : mm : ss Z\\ ' , LocaleUtils.parse ( \\'de\\ ' ) ) ; assertSameDate ( \\'Do. , 0 Dez . 0 00:00:00 -0800\\ ' , \\ ' E , d MMM yyyy HH : mm : ss Z\\ ' , LocaleUtils.parse ( \\'de\\ ' ) ) ; assertSameDate ( \\'Fr. , 0 Dez . 0 00:00:00 -0800\\ ' , \\ ' E , d MMM yyyy HH : mm : ss Z\\ ' , LocaleUtils.parse ( \\'de\\ ' ) ) ; DateTime dateTimeNow = DateTime.now ( DateTimeZone.UTC ) ; ZonedDateTime javaTimeNow = Instant.ofEpochMilli ( dateTimeNow.getMillis ( ) ) .atZone ( ZoneOffset.UTC ) ; assertSamePrinterOutput ( \\ ' E , d MMM yyyy HH : mm : ss Z\\ ' , LocaleUtils.parse ( \\'de\\ ' ) , javaTimeNow , dateTimeNow ) ; } If dirty WriteStateException occurred , it 's only safe to proceed if there is subsequent successful write of metadata and Manifest . We prefer to break here , not to over complicate test logic . See also MetaDataStateFormat # testFailRandomlyAndReadAnyState , that does not break . TODO : Deprecate and remove this setting must capture after snapshotting operations to ensure this MUS is at least the highest MUS of any of these operations . must capture IndexMetaData after snapshotting operations to ensure the returned mapping version is at least as up-to-date as the mapping version that these operations used . Here we must not use IndexMetaData from ClusterService for we expose a new cluster state to ClusterApplier ( s ) before exposing it in the ClusterService . if wait_for_metadata_version timeout , the response is empty ask for the next version ."
      }
    }
  }
}
